# ====================================================================
# QUADRUPED SELF-RECOVERY CONFIG - UNITREE GO2 + MUJOCO
# Based on: "Self-Recovery of Quadrupedal Robot Using DRL" (2024)
# Robot: Unitree Go2 (instead of A1)
# Simulator: MuJoCo (instead of PyBullet)
# ====================================================================

# ==================== ROBOT CONFIGURATION ====================
robot:
  name: "unitree_go2"

  # MuJoCo model path
  # Using model from root project directory
  model_path: "../../go2-sr-ppo-dl/environment/assets/unitree_go2/scene.xml"

  num_joints: 12

  # Heights (from paper Fig. 3)
  # Note: Go2 dimensions may differ slightly from A1
  max_height: 0.34 # Paper: "0.34 meter is the max height"
  target_height: 0.31 # Paper: "threshold at 0.31 meter"

# ==================== OBSERVATION SPACE ====================
# Paper Table I: 30-dimensional observation
observation:
  size: 30
  components:
    joint_positions: 12 # indices 0-11
    joint_velocities: 12 # indices 12-23
    base_orientation: 3 # indices 24-26 (θ_B = R^-1 · g)
    base_angular_velocity: 3 # indices 27-29

  # Add sensor noise (paper: "to replicate real robot")
  add_noise: true
  noise:
    joint_positions: 0.1 # ±0.1 rad
    joint_velocities: 1.0 # ±1.0 rad/s
    base_angular_velocity: 0.2 # ±0.2 rad/s

  # Normalization ranges (eq. 7: map to [-1, 1])
  # These are defaults - run extract_go2_params.py to get exact values
  normalization:
    joint_pos_min: -3.14
    joint_pos_max: 3.14
    joint_vel_min: -30.0
    joint_vel_max: 30.0
    base_orient_min: -1.0 # Already normalized (unit vector)
    base_orient_max: 1.0
    base_ang_vel_min: -10.0
    base_ang_vel_max: 10.0

# ==================== ACTION SPACE ====================
# Paper: "12 nodes, each representing the joint position command"
action:
  size: 12
  type: "continuous"
  range: [-1.0, 1.0] # Normalized, then rescaled to joint limits
  smoothing: 0.3 # EMA smoothing in env step() - slightly reduced for responsiveness
  scale: 1.0 # FULL joint range - robot needs full motion to flip over

# ==================== PD CONTROLLER ====================
# Paper Fig. 1: Policy outputs target positions to PD controller
controller:
  type: "pd"
  kp: 30.0 # Position gain - Lower for smoother control
  kd: 5.0 # Derivative gain - Higher for damping oscillations
  max_torque: 23.5 # Nm - Go2 actuator limit

# ==================== REWARD FUNCTION ====================
# Paper Table II & III, Eq. (9)
# IMPORTANT: All individual rewards are now in [0,1] range
reward:
  weights:
    # Always active rewards
    w1: 0.3 # R_h: Height reward
    w2: 0.3 # R_g: Orientation reward
    # Curriculum rewards (only when R_g > threshold)
    w3: 0.3 # R_h_cl: Height reward again
    w4: 0.6 # R_jp: Joint position reward
    w5: 0.1 # R_fc: Foot contact reward
    # Smoothness/regularization - INCREASED to penalize jerky movements
    w6: 0.05 # R_ad: Action difference (HIGH - smooth actions)
    w7: 0.005 # R_v: Joint velocity cost (penalize fast joint movements)
    w8: 0.05   # R_vb: Base linear velocity 
  curriculum:
    enabled: true
    orientation_threshold: 0.4

ppo:
  batch_size: 32
  learning_rate: 3.3e-4
  gamma: 0.97
  ent_coef: 2.0e-5
  gae_lambda: 0.95
  vf_coef: 0.3
  max_grad_norm: 0.7
  n_steps: 1024
  n_epochs: 10
  clip_range: 0.2
  policy_kwargs:
    net_arch: [256, 256, 16]
    activation_fn: "relu"

  verbose: 1
  tensorboard_log: "./logs/"
training:
  num_parallel_envs: 12
  max_episode_steps: 512
  drop_height: 0.5
  random_orientation: true
  random_joint_positions: true
  total_episodes: 300
  total_timesteps: 2_686_400
  save_freq: 100_000
  eval_freq: 25_000
  n_eval_episodes: 10
  log_interval: 5
  
simulation:
  engine: "mujoco"
  n_substeps: 10  
  early_termination: false
paths:
  checkpoints: "./checkpoints/"
  logs: "./logs/"
  videos: "./videos/"